{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3ae55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cccca6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shannonturing/miniconda3/envs/confit_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/shannonturing/miniconda3/envs/confit_env/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "4.39.3\n",
      "0.10.0\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "import transformers\n",
    "import peft\n",
    "# print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "print(peft.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a5d23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab00834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, get_peft_model\n",
    "from peft.utils.other import fsdp_auto_wrap_policy\n",
    "from transformers import EsmForMaskedLM, EsmTokenizer, EsmConfig\n",
    "import os\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import accelerate\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from confit.data_utils import Mutation_Set, split_train, sample_data\n",
    "from confit.stat_utils import spearman, compute_score, BT_loss, KLloss\n",
    "\n",
    "from Bio import SeqIO\n",
    "from transformers import EsmTokenizer\n",
    "\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6458a0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers==4.39.3 peft==0.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07d6c293",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7713972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv(\"data/BRCA1_HUMAN_Fields2015-e3/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc972125",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>log_fitness</th>\n",
       "      <th>n_mut</th>\n",
       "      <th>PID</th>\n",
       "      <th>mutated_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1271.000000</td>\n",
       "      <td>1271.000000</td>\n",
       "      <td>1271.0</td>\n",
       "      <td>1271.000000</td>\n",
       "      <td>1271.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>693.125098</td>\n",
       "      <td>0.546818</td>\n",
       "      <td>1.0</td>\n",
       "      <td>693.125098</td>\n",
       "      <td>52.129819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>398.213603</td>\n",
       "      <td>0.484519</td>\n",
       "      <td>0.0</td>\n",
       "      <td>398.213603</td>\n",
       "      <td>22.110417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.437145</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>348.500000</td>\n",
       "      <td>0.237671</td>\n",
       "      <td>1.0</td>\n",
       "      <td>348.500000</td>\n",
       "      <td>33.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>697.000000</td>\n",
       "      <td>0.520609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>697.000000</td>\n",
       "      <td>51.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1040.500000</td>\n",
       "      <td>0.834807</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1040.500000</td>\n",
       "      <td>71.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1381.000000</td>\n",
       "      <td>2.791509</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1381.000000</td>\n",
       "      <td>90.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  log_fitness   n_mut          PID  mutated_position\n",
       "count  1271.000000  1271.000000  1271.0  1271.000000       1271.000000\n",
       "mean    693.125098     0.546818     1.0   693.125098         52.129819\n",
       "std     398.213603     0.484519     0.0   398.213603         22.110417\n",
       "min       0.000000    -0.437145     1.0     0.000000         15.000000\n",
       "25%     348.500000     0.237671     1.0   348.500000         33.000000\n",
       "50%     697.000000     0.520609     1.0   697.000000         51.000000\n",
       "75%    1040.500000     0.834807     1.0  1040.500000         71.000000\n",
       "max    1381.000000     2.791509     1.0  1381.000000         90.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b3e5fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>seq</th>\n",
       "      <th>log_fitness</th>\n",
       "      <th>n_mut</th>\n",
       "      <th>mutant</th>\n",
       "      <th>PID</th>\n",
       "      <th>mutated_position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>MDLSALRVEEVQNVIAAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>0.908475</td>\n",
       "      <td>1</td>\n",
       "      <td>N16A</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MDLSALRVEEVQNVICAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>0.156238</td>\n",
       "      <td>1</td>\n",
       "      <td>N16C</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>MDLSALRVEEVQNVIEAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>1.287431</td>\n",
       "      <td>1</td>\n",
       "      <td>N16E</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>MDLSALRVEEVQNVIDAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>1.074311</td>\n",
       "      <td>1</td>\n",
       "      <td>N16D</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>MDLSALRVEEVQNVIGAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>1.177073</td>\n",
       "      <td>1</td>\n",
       "      <td>N16G</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                seq  log_fitness  \\\n",
       "0           0  MDLSALRVEEVQNVIAAMQKILECPICLELIKEPVSTKCDHIFCKF...     0.908475   \n",
       "1           1  MDLSALRVEEVQNVICAMQKILECPICLELIKEPVSTKCDHIFCKF...     0.156238   \n",
       "2           2  MDLSALRVEEVQNVIEAMQKILECPICLELIKEPVSTKCDHIFCKF...     1.287431   \n",
       "3           3  MDLSALRVEEVQNVIDAMQKILECPICLELIKEPVSTKCDHIFCKF...     1.074311   \n",
       "4           4  MDLSALRVEEVQNVIGAMQKILECPICLELIKEPVSTKCDHIFCKF...     1.177073   \n",
       "\n",
       "   n_mut mutant  PID  mutated_position  \n",
       "0      1   N16A    0                15  \n",
       "1      1   N16C    1                15  \n",
       "2      1   N16E    2                15  \n",
       "3      1   N16D    3                15  \n",
       "4      1   N16G    4                15  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2463f4",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "915b9636",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"BRCA1_HUMAN_Fields2015-e3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1827e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f'data/{dataset_name}'\n",
    "data = pd.read_csv(f\"{data_dir}/data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0390d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wild type\n",
    "wt_path = f\"{data_dir}/wt.fasta\"\n",
    "wt_seq = str(next(SeqIO.parse(wt_path, \"fasta\")).seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17f11514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wild-type sequence length: 110\n"
     ]
    }
   ],
   "source": [
    "print(f'Wild-type sequence length: {len(wt_seq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50b4b19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDLSALRVEEVQNVINAMQKILECPICLELIKEPVSTKCDHIFCKFCMLK\n"
     ]
    }
   ],
   "source": [
    "print(wt_seq[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15946bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load VAE\n",
    "vae_elbo = pd.read_csv(f\"{data_dir}/vae_elbo.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e295be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq</th>\n",
       "      <th>PID</th>\n",
       "      <th>elbo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MDLSALRVEEVQNVIAAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MDLSALRVEEVQNVICAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.123850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MDLSALRVEEVQNVIEAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.151109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MDLSALRVEEVQNVIDAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.040975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MDLSALRVEEVQNVIGAMQKILECPICLELIKEPVSTKCDHIFCKF...</td>\n",
       "      <td>4</td>\n",
       "      <td>-1.026119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 seq  PID      elbo\n",
       "0  MDLSALRVEEVQNVIAAMQKILECPICLELIKEPVSTKCDHIFCKF...    0  0.629923\n",
       "1  MDLSALRVEEVQNVICAMQKILECPICLELIKEPVSTKCDHIFCKF...    1 -2.123850\n",
       "2  MDLSALRVEEVQNVIEAMQKILECPICLELIKEPVSTKCDHIFCKF...    2  0.151109\n",
       "3  MDLSALRVEEVQNVIDAMQKILECPICLELIKEPVSTKCDHIFCKF...    3 -0.040975\n",
       "4  MDLSALRVEEVQNVIGAMQKILECPICLELIKEPVSTKCDHIFCKF...    4 -1.026119"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_elbo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae1d97c",
   "metadata": {},
   "source": [
    "### Prepare Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dd9bfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import login\n",
    "# login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2790cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm1b_t33_650M_UR50S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a15e6765",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shannonturing/miniconda3/envs/confit_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1955: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/shannonturing/miniconda3/envs/confit_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# \"facebook/esm1v_t33_650M_UR90S_1\" not working\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t48_15B_UR50D\", use_auth_token=True)\n",
    "seq_len = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ea2e25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = list(data[\"seq\"])\n",
    "fitness = torch.tensor(data[\"log_fitness\"].values, dtype=torch.float32)\n",
    "positions = [[int(p) for p in str(pos).split(',')] if isinstance(pos, str) else [pos] for pos in data['mutated_position']]\n",
    "pids = data[\"PID\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46953e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1271, 1271, 1271, 1271)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences), len(fitness), len(positions), len(pids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f8203b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21e021",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c50ac39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mutation_Set(Dataset):\n",
    "    def __init__(self, data, fname, tokenizer, sep_len=1024):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = sep_len\n",
    "        self.seq, self.attention_mask = tokenizer(list(self.data['seq']), padding='max_length',\n",
    "                                                  truncation=True,\n",
    "                                                  max_length=self.seq_len).values()\n",
    "        wt_path = os.path.join('data', fname, 'wt.fasta')\n",
    "        for seq_record in SeqIO.parse(wt_path, \"fasta\"):\n",
    "            wt = str(seq_record.seq)\n",
    "        target = [wt]*len(self.data)\n",
    "        self.target, self.tgt_mask = tokenizer(target, padding='max_length', truncation=True,\n",
    "                                               max_length=self.seq_len).values()\n",
    "        self.score = torch.tensor(np.array(self.data['log_fitness']))\n",
    "        self.pid = np.asarray(data['PID'])\n",
    "\n",
    "        if type(list(self.data['mutated_position'])[0]) != str:\n",
    "            self.position = [[u] for u in self.data['mutated_position']]\n",
    "\n",
    "        else:\n",
    "\n",
    "            temp = [u.split(',') for u in self.data['mutated_position']]\n",
    "            self.position = []\n",
    "            for u in temp:\n",
    "                pos = [int(v) for v in u]\n",
    "                self.position.append(pos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.seq[idx], self.attention_mask[idx], self.target[idx],self.tgt_mask[idx] ,self.position[idx], self.score[idx], self.pid[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.score)\n",
    "\n",
    "    def collate_fn(self, data):\n",
    "        seq = torch.tensor(np.array([u[0] for u in data]))\n",
    "        att_mask = torch.tensor(np.array([u[1] for u in data]))\n",
    "        tgt = torch.tensor(np.array([u[2] for u in data]))\n",
    "        tgt_mask = torch.tensor(np.array([u[3] for u in data]))\n",
    "        pos = [torch.tensor(u[4]) for u in data]\n",
    "        score = torch.tensor(np.array([u[5] for u in data]), dtype=torch.float32)\n",
    "        pid = torch.tensor(np.array([u[6] for u in data]))\n",
    "        return seq, att_mask, tgt, tgt_mask, pos, score, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ddde2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Mutation_Set(data, dataset_name, tokenizer, sep_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "40c90f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1271)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.attention_mask[0]), len(dataset.attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f65c15a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(dataset_name, seed, shot, frac=0.2):\n",
    "    '''\n",
    "    sample the train data and test data\n",
    "    :param seed: sample seed\n",
    "    :param frac: the fraction of testing data, default to 0.2\n",
    "    :param shot: the size of training data\n",
    "    '''\n",
    "\n",
    "    data = pd.read_csv(f'data/{dataset_name}/data.csv', index_col=0)\n",
    "    test_data = data.sample(frac=frac, random_state=seed)\n",
    "    train_data = data.drop(test_data.index)\n",
    "    \n",
    "    # low-N training\n",
    "    # prepares the few labeled mutants for fine-tuning while leaving the rest for testing or validation\n",
    "    kshot_data = train_data.sample(n=shot, random_state=seed)\n",
    "    \n",
    "    assert len(kshot_data) == shot, (\n",
    "        f'expected {shot} train examples, received {len(train_data)}')\n",
    "\n",
    "    kshot_data.to_csv(f'data/{dataset_name}/train.csv')\n",
    "    test_data.to_csv(f'data/{dataset_name}/test.csv')\n",
    "\n",
    "\n",
    "def split_train(dataset_name):\n",
    "    '''\n",
    "    five equal split training data, one of which will be used as validation set when training ConFit\n",
    "    '''\n",
    "    train = pd.read_csv(f'data/{dataset_name}/train.csv', index_col=0)\n",
    "    tlen = int(np.ceil(len(train) / 5))\n",
    "    start = 0\n",
    "    for i in range(1, 5):\n",
    "        csv = train[start:start + tlen]\n",
    "        start += tlen\n",
    "        csv.to_csv(f'data/{dataset_name}/train_{i}.csv')\n",
    "    csv = train[start:]\n",
    "    csv.to_csv(f'data/{dataset_name}/train_{5}.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def spearman(y_pred, y_true):\n",
    "    if np.var(y_pred) < 1e-6 or np.var(y_true) < 1e-6:\n",
    "        return 0.0\n",
    "    return spearmanr(y_pred, y_true)[0]\n",
    "\n",
    "def compute_stat(sr):\n",
    "    sr = np.asarray(sr)\n",
    "    mean = np.mean(sr)\n",
    "    std = np.std(sr)\n",
    "    sr = (sr,)\n",
    "    ci = list(bootstrap(sr, np.mean).confidence_interval)\n",
    "    return mean, std, ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "557be2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BT_loss(scores, golden_score):\n",
    "    loss = torch.tensor(0.)\n",
    "    loss = loss.cuda()\n",
    "    for i in range(len(scores)):\n",
    "        for j in range(i, len(scores)):\n",
    "            if golden_score[i] > golden_score[j]:\n",
    "                loss += torch.log(1+torch.exp(scores[j]-scores[i]))\n",
    "            else:\n",
    "                loss += torch.log(1+torch.exp(scores[i]-scores[j]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "def KLloss(logits, logits_reg, seq, att_mask):\n",
    "\n",
    "    creterion_reg = torch.nn.KLDivLoss(reduction='mean')\n",
    "    batch_size = int(seq.shape[0])\n",
    "\n",
    "    loss = torch.tensor(0.)\n",
    "    loss = loss.cuda()\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    probs_reg = torch.softmax(logits_reg, dim=-1)\n",
    "    for i in range(batch_size):\n",
    "\n",
    "        probs_i = probs[i]\n",
    "        probs_reg_i = probs_reg[i]\n",
    "\n",
    "\n",
    "        seq_len = torch.sum(att_mask[i])\n",
    "\n",
    "        reg = probs_reg_i[torch.arange(0, seq_len), seq[i, :seq_len]]\n",
    "        pred = probs_i[torch.arange(0, seq_len), seq[i, :seq_len]]\n",
    "\n",
    "        loss += creterion_reg(reg.log(), pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1919690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training size\n",
    "shot = 100\n",
    "seed = 0\n",
    "sample_data(dataset_name, seed, shot)\n",
    "split_train(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3a880d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(f'data/{dataset_name}/train.csv', index_col=0)\n",
    "val_data = pd.read_csv(f'data/{dataset_name}/train_1.csv', index_col=0)\n",
    "test_data = pd.read_csv(f'data/{dataset_name}/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94d8b032",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Mutation_Set(train_data, dataset_name, tokenizer, sep_len=seq_len)\n",
    "val_dataset = Mutation_Set(val_data, dataset_name, tokenizer, sep_len=seq_len)\n",
    "test_dataset = Mutation_Set(test_data, dataset_name, tokenizer, sep_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "390a073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                          collate_fn=train_dataset.collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False,\n",
    "                        collate_fn=val_dataset.collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False,\n",
    "                         collate_fn=test_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a5cfd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "58a5b789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, torch.Size([8, 1024]), torch.Size([8, 1024]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch), batch[0].shape, batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ab68e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shannonturing/miniconda3/envs/confit_env/lib/python3.9/site-packages/transformers/modeling_utils.py:2875: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/Users/shannonturing/miniconda3/envs/confit_env/lib/python3.9/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,351,680 || all params: 653,710,774 || trainable%: 0.2067703415272149\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/esm1b_t33_650M_UR50S\"\n",
    "base_model = EsmForMaskedLM.from_pretrained(model_name, \n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            # low_cpy_mem_usage=True,\n",
    "                                            use_auth_token=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"])\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3504fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 7/7 [01:42<00:00, 14.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,864,320 || all params: 15,142,217,634 || trainable%: 0.0519363820418327\n"
     ]
    }
   ],
   "source": [
    "model_name = \"facebook/esm2_t48_15B_UR50D\"\n",
    "base_model = EsmForMaskedLM.from_pretrained(model_name, \n",
    "                                            torch_dtype=torch.float16,\n",
    "                                            # low_cpy_mem_usage=True,\n",
    "                                            use_auth_token=True)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"value\"])\n",
    "\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d62332a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): EsmForMaskedLM(\n",
       "      (esm): EsmModel(\n",
       "        (embeddings): EsmEmbeddings(\n",
       "          (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
       "        )\n",
       "        (encoder): EsmEncoder(\n",
       "          (layer): ModuleList(\n",
       "            (0-32): 33 x EsmLayer(\n",
       "              (attention): EsmAttention(\n",
       "                (self): EsmSelfAttention(\n",
       "                  (query): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (value): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1280, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1280, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): EsmSelfOutput(\n",
       "                  (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (intermediate): EsmIntermediate(\n",
       "                (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "              )\n",
       "              (output): EsmOutput(\n",
       "                (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (contact_head): EsmContactPredictionHead(\n",
       "          (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "          (activation): Sigmoid()\n",
       "        )\n",
       "      )\n",
       "      (lm_head): EsmLMHead(\n",
       "        (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (decoder): Linear(in_features=1280, out_features=33, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "accelerator = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "lambda_reg = 0.1\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7844d3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 30.16 GB, other allocations: 22.73 MB, max allowed: 30.19 GB). Tried to allocate 20.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m seq, mask, wt, wt_mask, golden_score, pid \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39mto(device), mask\u001b[38;5;241m.\u001b[39mto(device), wt\u001b[38;5;241m.\u001b[39mto(device), wt_mask\u001b[38;5;241m.\u001b[39mto(device), golden_score\u001b[38;5;241m.\u001b[39mto(device), pid\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m pos \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pos]\n\u001b[0;32m---> 10\u001b[0m score, logits \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m l_bt \u001b[38;5;241m=\u001b[39m BT_loss(score, golden_score)\n\u001b[1;32m     12\u001b[0m l_reg \u001b[38;5;241m=\u001b[39m KLloss(logits, logits, seq, mask)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/PhD journey/fine-tune-protein lm/ConFit/confit/stat_utils.py:42\u001b[0m, in \u001b[0;36mcompute_score\u001b[0;34m(model, seq, mask, wt, pos, tokenizer)\u001b[0m\n\u001b[1;32m     39\u001b[0m     mut_pos \u001b[38;5;241m=\u001b[39m pos[i]\n\u001b[1;32m     40\u001b[0m     mask_seq[i, mut_pos\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m m_id\n\u001b[0;32m---> 42\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m logits \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     44\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog_softmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/peft/peft_model.py:1129\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_peft_forward_hooks(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1128\u001b[0m         kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspecial_peft_forward_args}\n\u001b[0;32m-> 1129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1132\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1134\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1135\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1140\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1142\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/peft/tuners/tuners_utils.py:161\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py:1021\u001b[0m, in \u001b[0;36mEsmForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1008\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mesm(\n\u001b[1;32m   1009\u001b[0m     input_ids,\n\u001b[1;32m   1010\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1018\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1019\u001b[0m )\n\u001b[1;32m   1020\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1021\u001b[0m prediction_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py:1059\u001b[0m, in \u001b[0;36mEsmLMHead.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m   1057\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(features)\n\u001b[1;32m   1058\u001b[0m x \u001b[38;5;241m=\u001b[39m gelu(x)\n\u001b[0;32m-> 1059\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# project back to size of vocabulary with bias\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/modules/normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/confit_env/lib/python3.9/site-packages/torch/nn/functional.py:2900\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2892\u001b[0m         layer_norm,\n\u001b[1;32m   2893\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2898\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m   2899\u001b[0m     )\n\u001b[0;32m-> 2900\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2901\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[1;32m   2902\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 30.16 GB, other allocations: 22.73 MB, max allowed: 30.19 GB). Tried to allocate 20.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        seq, mask, wt, wt_mask, pos, golden_score, pid = batch\n",
    "        seq, mask, wt, wt_mask, golden_score, pid = seq.to(device), mask.to(device), wt.to(device), wt_mask.to(device), golden_score.to(device), pid.to(device)\n",
    "        pos = [p.to(device) for p in pos]\n",
    "        score, logits = compute_score(model, seq, mask, wt, pos, tokenizer)\n",
    "        l_bt = BT_loss(score, golden_score)\n",
    "        l_reg = KLloss(logits, logits, seq, mask)\n",
    "        loss = l_bt + lambda_reg * l_reg\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f'Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}')\n",
    "    \n",
    "    val_sr = evaluate(model, val_loader, tokenizer, None)\n",
    "    print(f'Epoch {epoch + 1}, Validation Spearman: {val_sr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11af946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173b6819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "confit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
